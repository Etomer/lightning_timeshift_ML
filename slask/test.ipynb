{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- Notebook code to have the same import behavior as .py files --------------\n",
    "#%reset -f\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import os, sys\n",
    "if 'notebook_path' not in vars():\n",
    "    notebook_path = os.getcwd()\n",
    "os.chdir(os.path.join(notebook_path,\"..\"))\n",
    "sys.path.append(os.getcwd())\n",
    "# -----------------------------------------------------------------------------------\n",
    "#from src.data_modules.impulse_response import ImpulseResponseDataModule\n",
    "\n",
    "import h5py\n",
    "import lightning as L\n",
    "from src.data_modules import MovingImpulseResponseDataModule\n",
    "from src.cnn_model import cnn_model\n",
    "#ImpulseResponseDataModule()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/usr/lib/python3/dist-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (2.1.0) or chardet (5.2.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "/usr/lib/python3/dist-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (2.1.0) or chardet (5.2.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33merik-tegler\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20240103_171020-53e9qzwn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/erik-tegler/lightning_logs/runs/53e9qzwn' target=\"_blank\">comic-snowflake-1</a></strong> to <a href='https://wandb.ai/erik-tegler/lightning_logs' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/erik-tegler/lightning_logs' target=\"_blank\">https://wandb.ai/erik-tegler/lightning_logs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/erik-tegler/lightning_logs/runs/53e9qzwn' target=\"_blank\">https://wandb.ai/erik-tegler/lightning_logs/runs/53e9qzwn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name    | Type             | Params\n",
      "---------------------------------------------\n",
      "0 | loss_fn | CrossEntropyLoss | 0     \n",
      "1 | thinker | Sequential       | 9.1 M \n",
      "2 | cnn     | Sequential       | 194 K \n",
      "---------------------------------------------\n",
      "9.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "9.3 M     Total params\n",
      "37.126    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 76/76 [00:02<00:00, 26.40it/s, v_num=qzwn]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 76/76 [00:03<00:00, 19.08it/s, v_num=qzwn]\n"
     ]
    }
   ],
   "source": [
    "data_path = \"./data/datasets/moving_dataset_small.hdf5\"\n",
    "sound_dir = \"./data/reference_data/reference_sounds/\"\n",
    "data_module = MovingImpulseResponseDataModule(data_path,sound_dir,batch_size=2)\n",
    "\n",
    "model = cnn_model()\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "wandb_logger = WandbLogger(log_model=\"all\")\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=50,\n",
    "    accelerator=\"cuda\",\n",
    "    devices=1,\n",
    "    logger=wandb_logger,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data/datasets/example_moving.hdf5\"\n",
    "sound_dir = \"./data/reference_data/reference_sounds/\"\n",
    "data_module = MovingImpulseResponseDataModule(data_path,sound_dir,batch_size=5)\n",
    "\n",
    "\n",
    "\n",
    "# data_module.setup()\n",
    "# for i in data_module.train_dataloader():\n",
    "#     X = i[0].to(\"cuda\")\n",
    "#     break\n",
    "# len(data_module.train_dataloader())\n",
    "# #print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 51, 10, 1600)\n",
      "<KeysViewHDF5 ['fs', 'n_mics', 'n_rooms', 'reflection_coeff', 'rir_len', 'room_max_size', 'room_min_size', 'scatter_coeff', 'sound_source_locations', 'sound_source_max_move', 'speed_of_sound', 'target_path']>\n"
     ]
    }
   ],
   "source": [
    "data_path = \"./data/datasets/moving_dataset_small.hdf5\"\n",
    "with h5py.File(data_path, \"r\") as f:\n",
    "    print(f[\"input\"].shape)\n",
    "    print(f.attrs.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([680])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(data_module.train_dataloader()))\n",
    "X,y = batch\n",
    "\n",
    "import glob\n",
    "import torch\n",
    "\n",
    "from scipy.io import wavfile\n",
    "sample_length: int = 10000 #number of samples\n",
    "max_freq: float = 4000.0 # Hz\n",
    "batch_size : int = 1\n",
    "n_mics_per_batch = 17\n",
    "max_shift : float = 500 #samples\n",
    "n_shift_bins : int = 500\n",
    "validation_percentage = 0.05\n",
    "\n",
    "# remove\n",
    "dataset_metadata = data_module.h5file.attrs\n",
    "dataset = data_module.dataset\n",
    "sound_files = glob.glob(sound_dir + \"*.wav\")\n",
    "\n",
    "#const\n",
    "rir_len = dataset_metadata[\"rir_len\"]\n",
    "batch_size = X.shape[0]\n",
    "n_sound_positions = X.shape[2]\n",
    "sim_sample_length = sample_length + rir_len # length used when simmulating sound, needs to be longer than the final sound to have echo in the entire sample\n",
    "\n",
    "\n",
    "# randomly select audio pieces\n",
    "\n",
    "signals = torch.zeros(batch_size,sim_sample_length)\n",
    "for batch_i in range(batch_size):\n",
    "    sound_file = sound_files[torch.randint(len(sound_files),(1,))]\n",
    "    fs, signal = wavfile.read(sound_file)\n",
    "    if fs != dataset_metadata[\"fs\"]:\n",
    "        raise Exception(\"Please use .wav with the same sampling frequency as the simmulated impulse responses sampling frequency (probably 16 kHz\")\n",
    "    start = torch.randint(len(signal) - sim_sample_length, (1,))\n",
    "    signals[batch_i,:] = torch.tensor(signal[start : start + sim_sample_length])\n",
    "piece_length = signals.shape[1] // n_sound_positions\n",
    "signals = signals[:,:piece_length*n_sound_positions].reshape(batch_size, n_sound_positions, piece_length) # splitting the sound into the different speaker positions\n",
    "#signals = torch.concatenate([signals, torch.zeros(signals.shape[0], signals.shape[1], rir_len)],dim=2)\n",
    "\n",
    "signals = torch.fft.irfft(torch.fft.rfft(X, signals.shape[2] + rir_len) * torch.fft.rfft(signals, signals.shape[2] + rir_len).unsqueeze(1))\n",
    "\n",
    "fin_signal = torch.zeros(batch_size, n_mics_per_batch, signals.shape[3] + piece_length*(n_sound_positions-1))\n",
    "for i in range(n_sound_positions):\n",
    "    fin_signal[:,:,i*piece_length:piece_length*i+signals.shape[3]] = signals[:,:,i]\n",
    "\n",
    "fin_signal = fin_signal[:,:,-sample_length:]\n",
    "max_freq_component = int(max_freq*sample_length/dataset_metadata[\"fs\"])\n",
    "\n",
    "fin_signal = torch.fft.rfft(fin_signal)[:,:,:max_freq_component].unsqueeze(2)\n",
    "fin_signal = torch.concatenate([torch.concatenate([fin_signal, fin_signal.roll(i + 1, 1)], dim=2) for i in range((n_mics_per_batch-1) // 2)],dim=1)  # organize sounds pairwise, \n",
    "# NOTE: if n_mics_per_batch is not odd, then we will compute all pairs of microphones except one\n",
    "fin_signal = fin_signal.view(batch_size * fin_signal.shape[1],2,fin_signal.shape[3])\n",
    "fin_signal = torch.concatenate([fin_signal.real, fin_signal.imag], dim=1)\n",
    "\n",
    "y = torch.concatenate([y - y.roll(i + 1, 1) for i in range((n_mics_per_batch-1) // 2)], dim=1).view(-1) # compute pairwise distance-difference\n",
    "y *= dataset_metadata[\"fs\"]/dataset_metadata[\"speed_of_sound\"] # rescale distance-difference to sample_difference\n",
    "\n",
    "bin_edges = torch.linspace(-max_shift,max_shift,n_shift_bins+1)\n",
    "bin_edges[0] = -float(\"inf\")\n",
    "bin_edges[-1] = float(\"inf\")\n",
    "y = (y.unsqueeze(1) < bin_edges).to(torch.long).argmax(dim=1) - 1  # Bins the values in y, since argmax finds first occurence where condition is met.\n",
    "\n",
    "\n",
    "\n",
    "#torch.fft.irfft(torch.fft.rfft().unsqueeze(1) * torch.fft.rfft(X, signals.shape[2]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "343.0"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_metadata[\"speed_of_sound\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_simulated_data(X, y):\n",
    "            \"\"\"\n",
    "            transform a tensor of impulse responses in different rooms into pairwise TimeEstimation-problems. Note (X and y should be on GPU)\n",
    "\n",
    "            \"\"\"\n",
    "            # pull a random sound\n",
    "            sound_paths = glob(os.path.join(reference_sound_folder, \"*.wav\"))\n",
    "            # simulate longer sound and then cut to the relevant piece\n",
    "            signals = np.zeros(\n",
    "                (X.shape[0], config[\"sample_length\"] + config[\"rir_len\"] - 1)\n",
    "            )\n",
    "            for i in range(X.shape[0]):\n",
    "                sound_path = sound_paths[np.random.randint(len(sound_paths))]\n",
    "                fs, signal = wavfile.read(sound_path)\n",
    "                start = np.random.randint(\n",
    "                    0, len(signal) - config[\"sample_length\"] - config[\"rir_len\"] - 1\n",
    "                )\n",
    "                signals[i, :] = signal[\n",
    "                    start : start + config[\"sample_length\"] + config[\"rir_len\"] - 1\n",
    "                ]\n",
    "            # signals = torch.tensor(signals).to(torch.float32).to(device).unsqueeze(1)\n",
    "            signals = torch.tensor(signals).to(torch.float32).unsqueeze(1)\n",
    "\n",
    "            q = torch.fft.irfft(\n",
    "                torch.fft.rfft(signals) * torch.fft.rfft(X, signals.shape[2])\n",
    "            )[\n",
    "                :, :, : config[\"sample_length\"]\n",
    "            ]  # compute the heard sound, and cut it to the right length\n",
    "            q = torch.fft.rfft(q)[\n",
    "                :, :, : config[\"max_freq\"]\n",
    "            ]  # cut frequencies which are too high\n",
    "            q = q.unsqueeze(2)\n",
    "            q = torch.concatenate(\n",
    "                [\n",
    "                    torch.concatenate([q, q.roll(i + 1, 1)], dim=2)\n",
    "                    for i in range(config[\"mics_per_batch\"] // 2)\n",
    "                ],\n",
    "                dim=1,\n",
    "            )  # organize sounds pairwise\n",
    "            q = q.view(\n",
    "                X.shape[0]\n",
    "                * (config[\"mics_per_batch\"] * (config[\"mics_per_batch\"] - 1))\n",
    "                // 2,\n",
    "                2,\n",
    "                -1,\n",
    "            )  # reshape so that each example is a row\n",
    "            X = torch.concatenate([q.real, q.imag], dim=1)\n",
    "            X /= (\n",
    "                X.std(dim=2).mean(dim=1).unsqueeze(1).unsqueeze(2) + 1e-5\n",
    "            )  # avoid dividing by 0\n",
    "            y = (\n",
    "                torch.concatenate(\n",
    "                    [\n",
    "                        y - y.roll(i + 1, 1)\n",
    "                        for i in range(config[\"mics_per_batch\"] // 2)\n",
    "                    ],\n",
    "                    dim=1,\n",
    "                ).view(-1)\n",
    "                * fs\n",
    "                / 343\n",
    "            )  # compute gt for all pairs\n",
    "            y = y_to_class_gt(y, config[\"guess_grid_size\"], config[\"max_shift\"]).to(\n",
    "                torch.long\n",
    "            )\n",
    "\n",
    "            return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"input\": shape (10, 51, 10, 1600), type \"<f4\">"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_module.dataset.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with h5py.File(\"./test.hdf5\",\"w\") as f:\n",
    "    f.create_dataset(\"metadata\",())\n",
    "    z = f[\"metadata\"]\n",
    "    #metadata = z.attrs\n",
    "    f.attrs[\"a\"] = 43\n",
    "    #metadata.create(\"a\",123)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7f226e654220>\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "with h5py.File(\"./data/datasets/example_moving.hdf5\",\"r\") as f:\n",
    "    X = f[\"input\"]\n",
    "    #torch.utils.data.IterableDataset(X)\n",
    "    print(DataLoader(X))\n",
    "    torch.utils.data.Subset(X,torch.arange(5))\n",
    "    #metadata = z.attrs\n",
    "    #for i in f.attrs:\n",
    "    #    print(f'{i} : {f.attrs[i]}')\n",
    "    #print(f.attrs[\"a\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': 1, 'y': 2}\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "def f(x,y):\n",
    "    print(locals())\n",
    "    print(vars()['x'])\n",
    "\n",
    "f(1,2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
