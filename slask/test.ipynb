{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------- Notebook code to have the same import behavior as .py files --------------\n",
    "#%reset -f\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import os, sys\n",
    "if 'notebook_path' not in vars():\n",
    "    notebook_path = os.getcwd()\n",
    "os.chdir(os.path.join(notebook_path,\"..\"))\n",
    "sys.path.append(os.getcwd())\n",
    "# -----------------------------------------------------------------------------------\n",
    "#from src.data_modules.impulse_response import ImpulseResponseDataModule\n",
    "\n",
    "import h5py\n",
    "import lightning as L\n",
    "from src.data_modules import MovingImpulseResponseDataModule\n",
    "from src.cnn_model import cnn_model\n",
    "#ImpulseResponseDataModule()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data/datasets/moving_dataset_small.hdf5\"\n",
    "sound_dir = \"./data/reference_data/reference_sounds/\"\n",
    "data_module = MovingImpulseResponseDataModule(data_path,sound_dir,batch_size=2)\n",
    "\n",
    "model = cnn_model()\n",
    "data_module.setup()\n",
    "X,y = next(iter(data_module.train_dataloader()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doppler augmentation. \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "#plt.figure(figsize=(16,6))\n",
    "#plt.plot(X[0,0,200:500],'.')\n",
    "\n",
    "class doppler_aug(object):\n",
    "\n",
    "    def __init__(self, max_rel_v = 3):\n",
    "        self.max_rel_v = max_rel_v\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        X,y = sample\n",
    "        factor = (2*self.max_rel_v*torch.rand(1) + 343 - self.max_rel_v)/343\n",
    "\n",
    "        freqs = X.shape[-1]\n",
    "        if factor < 1:\n",
    "            x = torch.arange(freqs)*factor\n",
    "            w_lower = 1 - (x - x.int())\n",
    "            X[:,0::2] = X[:,0::2,x.int()]*w_lower + (1 - w_lower)*X[:,0::2,x.int()+1]\n",
    "        else:\n",
    "            x = torch.arange(freqs)*factor\n",
    "            x = x[x < freqs - 1] # make sure we don't query freq values outside of input vector\n",
    "            w_lower = 1 - (x - x.int())\n",
    "            \n",
    "            X[:,0::2,:x.shape[0]] = X[:,0::2,x.int()]*w_lower + (1 - w_lower)*X[:,0::2,x.int()+1]\n",
    "            X[:,0::2,x.shape[0]:] = 0\n",
    "\n",
    "        return X,y\n",
    "\n",
    "class noise_aug(object):\n",
    "\n",
    "    def __init__(self, noise_ratio = 0.01):\n",
    "        self.noise_ratio = noise_ratio\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        X,y = sample\n",
    "        X += self.noise_ratio*X.std(dim=(1,2),keepdim=True)*torch.randn(X.shape)\n",
    "        return X,y\n",
    "\n",
    "\n",
    "#plt.plot(X[0,0,2000:2500],'.')\n",
    "\n",
    "\n",
    "\n",
    "#(torch.arange(2500)*factor).int()\n",
    "#(torch.arange(2500)*factor).int() + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise augmentation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# from pytorch_lightning.loggers import WandbLogger\n",
    "# wandb_logger = WandbLogger(log_model=\"all\")\n",
    "# trainer = L.Trainer(\n",
    "#     max_epochs=50,\n",
    "#     accelerator=\"cuda\",\n",
    "#     devices=1,\n",
    "#     logger=wandb_logger,\n",
    "# )\n",
    "# trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3200, 51, 10, 1600)\n",
      "<KeysViewHDF5 ['fs', 'n_mics', 'n_rooms', 'reflection_coeff', 'rir_len', 'room_max_size', 'room_min_size', 'scatter_coeff', 'sound_source_locations', 'sound_source_max_move', 'speed_of_sound', 'target_path']>\n",
      "torch.Size([30, 51, 10, 1600])\n",
      "torch.Size([30])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "data_path = \"./data/datasets/moving_dataset_medium.hdf5\"\n",
    "with h5py.File(data_path, \"r\") as f:\n",
    "    print(f[\"input\"].shape)\n",
    "    print(f.attrs.keys())\n",
    "    X = f[\"input\"]\n",
    "    print(torch.tensor(X[:30]).shape)\n",
    "    print(torch.tensor(X[:30]).std(dim=(1,2,3)).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([680])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(data_module.train_dataloader()))\n",
    "X,y = batch\n",
    "\n",
    "import glob\n",
    "import torch\n",
    "\n",
    "from scipy.io import wavfile\n",
    "sample_length: int = 10000 #number of samples\n",
    "max_freq: float = 4000.0 # Hz\n",
    "batch_size : int = 1\n",
    "n_mics_per_batch = 17\n",
    "max_shift : float = 500 #samples\n",
    "n_shift_bins : int = 500\n",
    "validation_percentage = 0.05\n",
    "\n",
    "# remove\n",
    "dataset_metadata = data_module.h5file.attrs\n",
    "dataset = data_module.dataset\n",
    "sound_files = glob.glob(sound_dir + \"*.wav\")\n",
    "\n",
    "#const\n",
    "rir_len = dataset_metadata[\"rir_len\"]\n",
    "batch_size = X.shape[0]\n",
    "n_sound_positions = X.shape[2]\n",
    "sim_sample_length = sample_length + rir_len # length used when simmulating sound, needs to be longer than the final sound to have echo in the entire sample\n",
    "\n",
    "\n",
    "# randomly select audio pieces\n",
    "\n",
    "signals = torch.zeros(batch_size,sim_sample_length)\n",
    "for batch_i in range(batch_size):\n",
    "    sound_file = sound_files[torch.randint(len(sound_files),(1,))]\n",
    "    fs, signal = wavfile.read(sound_file)\n",
    "    if fs != dataset_metadata[\"fs\"]:\n",
    "        raise Exception(\"Please use .wav with the same sampling frequency as the simmulated impulse responses sampling frequency (probably 16 kHz\")\n",
    "    start = torch.randint(len(signal) - sim_sample_length, (1,))\n",
    "    signals[batch_i,:] = torch.tensor(signal[start : start + sim_sample_length])\n",
    "piece_length = signals.shape[1] // n_sound_positions\n",
    "signals = signals[:,:piece_length*n_sound_positions].reshape(batch_size, n_sound_positions, piece_length) # splitting the sound into the different speaker positions\n",
    "#signals = torch.concatenate([signals, torch.zeros(signals.shape[0], signals.shape[1], rir_len)],dim=2)\n",
    "\n",
    "signals = torch.fft.irfft(torch.fft.rfft(X, signals.shape[2] + rir_len) * torch.fft.rfft(signals, signals.shape[2] + rir_len).unsqueeze(1))\n",
    "\n",
    "fin_signal = torch.zeros(batch_size, n_mics_per_batch, signals.shape[3] + piece_length*(n_sound_positions-1))\n",
    "for i in range(n_sound_positions):\n",
    "    fin_signal[:,:,i*piece_length:piece_length*i+signals.shape[3]] = signals[:,:,i]\n",
    "\n",
    "fin_signal = fin_signal[:,:,-sample_length:]\n",
    "max_freq_component = int(max_freq*sample_length/dataset_metadata[\"fs\"])\n",
    "\n",
    "fin_signal = torch.fft.rfft(fin_signal)[:,:,:max_freq_component].unsqueeze(2)\n",
    "fin_signal = torch.concatenate([torch.concatenate([fin_signal, fin_signal.roll(i + 1, 1)], dim=2) for i in range((n_mics_per_batch-1) // 2)],dim=1)  # organize sounds pairwise, \n",
    "# NOTE: if n_mics_per_batch is not odd, then we will compute all pairs of microphones except one\n",
    "fin_signal = fin_signal.view(batch_size * fin_signal.shape[1],2,fin_signal.shape[3])\n",
    "fin_signal = torch.concatenate([fin_signal.real, fin_signal.imag], dim=1)\n",
    "\n",
    "y = torch.concatenate([y - y.roll(i + 1, 1) for i in range((n_mics_per_batch-1) // 2)], dim=1).view(-1) # compute pairwise distance-difference\n",
    "y *= dataset_metadata[\"fs\"]/dataset_metadata[\"speed_of_sound\"] # rescale distance-difference to sample_difference\n",
    "\n",
    "bin_edges = torch.linspace(-max_shift,max_shift,n_shift_bins+1)\n",
    "bin_edges[0] = -float(\"inf\")\n",
    "bin_edges[-1] = float(\"inf\")\n",
    "y = (y.unsqueeze(1) < bin_edges).to(torch.long).argmax(dim=1) - 1  # Bins the values in y, since argmax finds first occurence where condition is met.\n",
    "\n",
    "\n",
    "\n",
    "#torch.fft.irfft(torch.fft.rfft().unsqueeze(1) * torch.fft.rfft(X, signals.shape[2]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "343.0"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_metadata[\"speed_of_sound\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_simulated_data(X, y):\n",
    "            \"\"\"\n",
    "            transform a tensor of impulse responses in different rooms into pairwise TimeEstimation-problems. Note (X and y should be on GPU)\n",
    "\n",
    "            \"\"\"\n",
    "            # pull a random sound\n",
    "            sound_paths = glob(os.path.join(reference_sound_folder, \"*.wav\"))\n",
    "            # simulate longer sound and then cut to the relevant piece\n",
    "            signals = np.zeros(\n",
    "                (X.shape[0], config[\"sample_length\"] + config[\"rir_len\"] - 1)\n",
    "            )\n",
    "            for i in range(X.shape[0]):\n",
    "                sound_path = sound_paths[np.random.randint(len(sound_paths))]\n",
    "                fs, signal = wavfile.read(sound_path)\n",
    "                start = np.random.randint(\n",
    "                    0, len(signal) - config[\"sample_length\"] - config[\"rir_len\"] - 1\n",
    "                )\n",
    "                signals[i, :] = signal[\n",
    "                    start : start + config[\"sample_length\"] + config[\"rir_len\"] - 1\n",
    "                ]\n",
    "            # signals = torch.tensor(signals).to(torch.float32).to(device).unsqueeze(1)\n",
    "            signals = torch.tensor(signals).to(torch.float32).unsqueeze(1)\n",
    "\n",
    "            q = torch.fft.irfft(\n",
    "                torch.fft.rfft(signals) * torch.fft.rfft(X, signals.shape[2])\n",
    "            )[\n",
    "                :, :, : config[\"sample_length\"]\n",
    "            ]  # compute the heard sound, and cut it to the right length\n",
    "            q = torch.fft.rfft(q)[\n",
    "                :, :, : config[\"max_freq\"]\n",
    "            ]  # cut frequencies which are too high\n",
    "            q = q.unsqueeze(2)\n",
    "            q = torch.concatenate(\n",
    "                [\n",
    "                    torch.concatenate([q, q.roll(i + 1, 1)], dim=2)\n",
    "                    for i in range(config[\"mics_per_batch\"] // 2)\n",
    "                ],\n",
    "                dim=1,\n",
    "            )  # organize sounds pairwise\n",
    "            q = q.view(\n",
    "                X.shape[0]\n",
    "                * (config[\"mics_per_batch\"] * (config[\"mics_per_batch\"] - 1))\n",
    "                // 2,\n",
    "                2,\n",
    "                -1,\n",
    "            )  # reshape so that each example is a row\n",
    "            X = torch.concatenate([q.real, q.imag], dim=1)\n",
    "            X /= (\n",
    "                X.std(dim=2).mean(dim=1).unsqueeze(1).unsqueeze(2) + 1e-5\n",
    "            )  # avoid dividing by 0\n",
    "            y = (\n",
    "                torch.concatenate(\n",
    "                    [\n",
    "                        y - y.roll(i + 1, 1)\n",
    "                        for i in range(config[\"mics_per_batch\"] // 2)\n",
    "                    ],\n",
    "                    dim=1,\n",
    "                ).view(-1)\n",
    "                * fs\n",
    "                / 343\n",
    "            )  # compute gt for all pairs\n",
    "            y = y_to_class_gt(y, config[\"guess_grid_size\"], config[\"max_shift\"]).to(\n",
    "                torch.long\n",
    "            )\n",
    "\n",
    "            return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"input\": shape (10, 51, 10, 1600), type \"<f4\">"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_module.dataset.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with h5py.File(\"./test.hdf5\",\"w\") as f:\n",
    "    f.create_dataset(\"metadata\",())\n",
    "    z = f[\"metadata\"]\n",
    "    #metadata = z.attrs\n",
    "    f.attrs[\"a\"] = 43\n",
    "    #metadata.create(\"a\",123)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7f226e654220>\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "with h5py.File(\"./data/datasets/example_moving.hdf5\",\"r\") as f:\n",
    "    X = f[\"input\"]\n",
    "    #torch.utils.data.IterableDataset(X)\n",
    "    print(DataLoader(X))\n",
    "    torch.utils.data.Subset(X,torch.arange(5))\n",
    "    #metadata = z.attrs\n",
    "    #for i in f.attrs:\n",
    "    #    print(f'{i} : {f.attrs[i]}')\n",
    "    #print(f.attrs[\"a\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': 1, 'y': 2}\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "def f(x,y):\n",
    "    print(locals())\n",
    "    print(vars()['x'])\n",
    "\n",
    "f(1,2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
